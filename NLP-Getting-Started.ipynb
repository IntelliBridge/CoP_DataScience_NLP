{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b88fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6b22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6b398",
   "metadata": {},
   "source": [
    "# Brief look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1194e6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3182</th>\n",
       "      <td>4568</td>\n",
       "      <td>emergency%20plan</td>\n",
       "      <td>Calgary,AB, Canada</td>\n",
       "      <td>The City has activated the Municipal Emergency...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5688</th>\n",
       "      <td>8118</td>\n",
       "      <td>rescued</td>\n",
       "      <td>Philadelphia, Pennsylvania USA</td>\n",
       "      <td>Homeless Man Traveling Across USA With 11 Stra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>7889</td>\n",
       "      <td>quarantined</td>\n",
       "      <td>china</td>\n",
       "      <td>Top link: Reddit's new content policy goes int...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>251</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>New Orleans, LA</td>\n",
       "      <td>Leading emergency services boss welcomes new a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>3394</td>\n",
       "      <td>demolition</td>\n",
       "      <td>US-PR</td>\n",
       "      <td>@Treyarch @DavidVonderhaar  bring back demolit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>4153</td>\n",
       "      <td>drown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'Save me from my self don't let me drown'.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>1543</td>\n",
       "      <td>bomb</td>\n",
       "      <td>Canada</td>\n",
       "      <td>@CranBoonitz So going to make any bomb threats...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>3001</td>\n",
       "      <td>dead</td>\n",
       "      <td>Milton Keynes</td>\n",
       "      <td>Can't believe Ross is dead???????? @emmerdale ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7419</th>\n",
       "      <td>10612</td>\n",
       "      <td>wounded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>7650</td>\n",
       "      <td>panic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The cool kids asked me if I wanted to hang out...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>9286</td>\n",
       "      <td>sunk</td>\n",
       "      <td>i beg vines sorry</td>\n",
       "      <td>@UnrealTouch fuck sake john Jesus my heart jus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>653</td>\n",
       "      <td>attack</td>\n",
       "      <td>Selena | Britney | Hilary</td>\n",
       "      <td>Demi stans really think Heart Attack sold 5/6 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>6003</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>United States</td>\n",
       "      <td>JKL issues Hazardous Weather Outlook (HWO)  ht...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4504</th>\n",
       "      <td>6402</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>Anderson, SC</td>\n",
       "      <td>hurricane?? sick!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6032</th>\n",
       "      <td>8622</td>\n",
       "      <td>seismic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>On Thursday at 00:25 we updated our #kml of 2D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>3099</td>\n",
       "      <td>deaths</td>\n",
       "      <td>Columbia Heights, MN</td>\n",
       "      <td>Walmart is taking steps to keep children safe ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>9822</td>\n",
       "      <td>trauma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hiroshima: They told me to paint my story: Eig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>7277</td>\n",
       "      <td>nuclear%20disaster</td>\n",
       "      <td>#goingdownthetoilet Illinois</td>\n",
       "      <td>CLOSING THEIR EYES TO DISASTER!  State Departm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>6835</td>\n",
       "      <td>loud%20bang</td>\n",
       "      <td>{Detailed}</td>\n",
       "      <td>@ToxicSavior_ -a loud bang. He froze on the sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>10862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id             keyword                        location  \\\n",
       "3182   4568    emergency%20plan              Calgary,AB, Canada   \n",
       "5688   8118             rescued  Philadelphia, Pennsylvania USA   \n",
       "5531   7889         quarantined                           china   \n",
       "175     251           ambulance                 New Orleans, LA   \n",
       "2358   3394          demolition                           US-PR   \n",
       "2890   4153               drown                             NaN   \n",
       "1068   1543                bomb                          Canada   \n",
       "2088   3001                dead                  Milton Keynes    \n",
       "7419  10612             wounded                             NaN   \n",
       "5361   7650               panic                             NaN   \n",
       "6494   9286                sunk              i beg vines sorry    \n",
       "450     653              attack       Selena | Britney | Hilary   \n",
       "4228   6003           hazardous                   United States   \n",
       "4504   6402           hurricane                    Anderson, SC   \n",
       "6032   8622             seismic                             NaN   \n",
       "2159   3099              deaths            Columbia Heights, MN   \n",
       "6853   9822              trauma                             NaN   \n",
       "5101   7277  nuclear%20disaster    #goingdownthetoilet Illinois   \n",
       "4803   6835         loud%20bang                      {Detailed}   \n",
       "7603  10862                 NaN                             NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "3182  The City has activated the Municipal Emergency...       1  \n",
       "5688  Homeless Man Traveling Across USA With 11 Stra...       0  \n",
       "5531  Top link: Reddit's new content policy goes int...       0  \n",
       "175   Leading emergency services boss welcomes new a...       0  \n",
       "2358  @Treyarch @DavidVonderhaar  bring back demolit...       0  \n",
       "2890         'Save me from my self don't let me drown'.       0  \n",
       "1068  @CranBoonitz So going to make any bomb threats...       0  \n",
       "2088  Can't believe Ross is dead???????? @emmerdale ...       0  \n",
       "7419  Police Officer Wounded Suspect Dead After Exch...       1  \n",
       "5361  The cool kids asked me if I wanted to hang out...       0  \n",
       "6494  @UnrealTouch fuck sake john Jesus my heart jus...       1  \n",
       "450   Demi stans really think Heart Attack sold 5/6 ...       0  \n",
       "4228  JKL issues Hazardous Weather Outlook (HWO)  ht...       1  \n",
       "4504                                  hurricane?? sick!       1  \n",
       "6032  On Thursday at 00:25 we updated our #kml of 2D...       0  \n",
       "2159  Walmart is taking steps to keep children safe ...       0  \n",
       "6853  Hiroshima: They told me to paint my story: Eig...       1  \n",
       "5101  CLOSING THEIR EYES TO DISASTER!  State Departm...       1  \n",
       "4803  @ToxicSavior_ -a loud bang. He froze on the sp...       0  \n",
       "7603  Officials say a quarantine is in place at an A...       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df2736",
   "metadata": {},
   "source": [
    "# Clean the text up a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a3db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.lower()) # convert to lowercase\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(\"#\", \"\")) # remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63942f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"http\" not in x])) # remove hypterlinks\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"@\" not in x])) # remove tags\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"a\" != x])) # remove a\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"the\" != x])) # remove the\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"an\" != x])) # remove an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff66ac27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5744</th>\n",
       "      <td>8202</td>\n",
       "      <td>riot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>discovered by \\n listen/buy riot on û_ blowma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>on plus side look at sky last night it was ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6675</th>\n",
       "      <td>9566</td>\n",
       "      <td>thunder</td>\n",
       "      <td>Decatur, GA</td>\n",
       "      <td>brings her alabama thunder back to attic septe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>298</td>\n",
       "      <td>annihilated</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>uribe just annihilated that baseball. mets</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>3470</td>\n",
       "      <td>derailed</td>\n",
       "      <td>USA</td>\n",
       "      <td>note there were no passengers on board when tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975</th>\n",
       "      <td>10005</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all of this energy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>8183</td>\n",
       "      <td>rescuers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>video: 'we're picking up bodies from water': r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6072</th>\n",
       "      <td>8674</td>\n",
       "      <td>sinkhole</td>\n",
       "      <td>San Diego California 92101</td>\n",
       "      <td>water main break disrupts trolley service sand...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>5928</td>\n",
       "      <td>harm</td>\n",
       "      <td>Portland, OR</td>\n",
       "      <td>no harm no foul and somebody needed to say it.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5696</th>\n",
       "      <td>8129</td>\n",
       "      <td>rescued</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heroes! springer spaniel &amp;amp; her dog dad res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>4912</td>\n",
       "      <td>exploded</td>\n",
       "      <td>Trost District</td>\n",
       "      <td>even you have to admit seras that sasha was cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>5803</td>\n",
       "      <td>hail</td>\n",
       "      <td>USA</td>\n",
       "      <td>strong thunderstorm 4 miles north of japton mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6796</th>\n",
       "      <td>9736</td>\n",
       "      <td>tragedy</td>\n",
       "      <td>Silicon Valley</td>\n",
       "      <td>tragedy of commons pertains to public ownershi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>2693</td>\n",
       "      <td>crush</td>\n",
       "      <td>NaN</td>\n",
       "      <td>men crush every fucking day???????????????????...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>3102</td>\n",
       "      <td>deaths</td>\n",
       "      <td>Top secret bunker</td>\n",
       "      <td>pls reduce cyclist deaths with compulsory high...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>4899</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>learn how i gained access to secrets of top ea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>5343</td>\n",
       "      <td>fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nothing like good fire</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963</th>\n",
       "      <td>8514</td>\n",
       "      <td>screaming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>camila's doing follow spree tonight im screami...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>1863</td>\n",
       "      <td>burned</td>\n",
       "      <td>Upper St Clair, PA</td>\n",
       "      <td>keithyy gettin burned outta blocks and on soci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>9831</td>\n",
       "      <td>trauma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>civil rights continued in 60s. and what about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>6015</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>too toxic...cancer....disease...hazardous wast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6522</th>\n",
       "      <td>9329</td>\n",
       "      <td>survive</td>\n",
       "      <td>EveryWhere</td>\n",
       "      <td>:: survive??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>7731</td>\n",
       "      <td>panicking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i feel like i should be panicking more as idk ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>10379</td>\n",
       "      <td>weapons</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no idea what this means. look at our violent c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>1890</td>\n",
       "      <td>burning</td>\n",
       "      <td>Black Canyon New River, AZ</td>\n",
       "      <td>flames visible from fire in tucson mountains: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      keyword                    location  \\\n",
       "5744   8202         riot                         NaN   \n",
       "35       53       ablaze                  London, UK   \n",
       "6675   9566      thunder                 Decatur, GA   \n",
       "211     298  annihilated                New York, NY   \n",
       "2411   3470     derailed                         USA   \n",
       "6975  10005      tsunami                         NaN   \n",
       "5734   8183     rescuers                         NaN   \n",
       "6072   8674     sinkhole  San Diego California 92101   \n",
       "4173   5928         harm                Portland, OR   \n",
       "5696   8129      rescued                         NaN   \n",
       "3436   4912     exploded              Trost District   \n",
       "4083   5803         hail                         USA   \n",
       "6796   9736      tragedy              Silicon Valley   \n",
       "1874   2693        crush                         NaN   \n",
       "2162   3102       deaths          Top secret bunker    \n",
       "3426   4899      explode                         NaN   \n",
       "3761   5343         fire                         NaN   \n",
       "5963   8514    screaming                         NaN   \n",
       "1290   1863       burned          Upper St Clair, PA   \n",
       "6859   9831       trauma                         NaN   \n",
       "4234   6015    hazardous                         NaN   \n",
       "6522   9329      survive                  EveryWhere   \n",
       "5417   7731    panicking                         NaN   \n",
       "7249  10379      weapons                         NaN   \n",
       "1309   1890      burning  Black Canyon New River, AZ   \n",
       "\n",
       "                                                   text  target  \n",
       "5744  discovered by \\n listen/buy riot on û_ blowma...       0  \n",
       "35    on plus side look at sky last night it was ablaze       0  \n",
       "6675  brings her alabama thunder back to attic septe...       0  \n",
       "211          uribe just annihilated that baseball. mets       0  \n",
       "2411  note there were no passengers on board when tr...       1  \n",
       "6975                                 all of this energy       0  \n",
       "5734  video: 'we're picking up bodies from water': r...       1  \n",
       "6072  water main break disrupts trolley service sand...       1  \n",
       "4173     no harm no foul and somebody needed to say it.       0  \n",
       "5696  heroes! springer spaniel &amp; her dog dad res...       1  \n",
       "3436  even you have to admit seras that sasha was cu...       0  \n",
       "4083  strong thunderstorm 4 miles north of japton mo...       1  \n",
       "6796  tragedy of commons pertains to public ownershi...       0  \n",
       "1874  men crush every fucking day???????????????????...       0  \n",
       "2162  pls reduce cyclist deaths with compulsory high...       1  \n",
       "3426  learn how i gained access to secrets of top ea...       0  \n",
       "3761                             nothing like good fire       1  \n",
       "5963  camila's doing follow spree tonight im screami...       0  \n",
       "1290  keithyy gettin burned outta blocks and on soci...       0  \n",
       "6859  civil rights continued in 60s. and what about ...       0  \n",
       "4234  too toxic...cancer....disease...hazardous wast...       1  \n",
       "6522                                       :: survive??       0  \n",
       "5417  i feel like i should be panicking more as idk ...       0  \n",
       "7249  no idea what this means. look at our violent c...       0  \n",
       "1309  flames visible from fire in tucson mountains: ...       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5c856",
   "metadata": {},
   "source": [
    "# We must turn text into mathematical representation\n",
    "There are a number of ways to do this, the most simple of which is a count vectorizor, where we simply count the number of times a word shows up in the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9c3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "## let's get counts for the first single tweet in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(df[\"text\"][0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706055bf",
   "metadata": {},
   "source": [
    "## Notice there are 12 unique words in the tweet (after cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27f3254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'our deeds are reason of this earthquake may allah forgive us all'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b959a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12)\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(example_train_vectors[0].todense().shape)\n",
    "print(example_train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0157e73",
   "metadata": {},
   "source": [
    "### Let's see what happens when we scale this up a bit \n",
    "We are now going to vectorize 20 tweets at once. This increases our __vocabulary__, which is an important concept in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1040d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(df[\"text\"][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d52bd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just got sent this photo from ruby alaska as smoke from wildfires pours into school '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747a54b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 145)\n",
      "[[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "print(example_train_vectors[4].todense().shape)\n",
    "print(example_train_vectors[4].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3897378",
   "metadata": {},
   "source": [
    "### Note there are 145 words in the \"Vocabulary\" of the embedding above. \n",
    "There are an infinite number of possible \"embeddings\". The particular embedding does not care about the order of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b80f78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(df['text']), pd.DataFrame(df['target']), test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f15b43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>pyrotechnic artwork by cai guo-qiang explodes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>don't worry i'm sure climate has nothing to do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6454</th>\n",
       "      <td>imagine school where suicide bombing is being ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "3486  pyrotechnic artwork by cai guo-qiang explodes ...\n",
       "3988  don't worry i'm sure climate has nothing to do...\n",
       "6454  imagine school where suicide bombing is being ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16867b",
   "metadata": {},
   "source": [
    "## Now that we have seen embeddings, let's embed the entire dataset.\n",
    "This will create our \"features\" for our machine learning models to learn on. Machine learning is just statistical equations, so it makes sense to force all the inputs to be numbers. One thing we should call out is, the training dataset defines the size of the vocabulary. How might this effect things in unexpected ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd55dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(X_train[\"text\"])\n",
    "\n",
    "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
    "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# i.e. that the train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(X_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1564f7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11751)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Vocabulary is 11751 words!\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors[0].todense().shape)\n",
    "print(train_vectors[0].todense())\n",
    "print(f\"Vocabulary is {train_vectors[0].todense().shape[1]} words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957b031",
   "metadata": {},
   "source": [
    "# Now that we have our features\n",
    "Let's train the actual model. For this we will use a very simply logistic regression model, which is the gold standard for simple binary classificaiton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47449e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylan.frizzell/COP/nlp-demo/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py:1141: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a6260",
   "metadata": {},
   "source": [
    "# Evaluate the performance on test set\n",
    "In a production setting there is usually a lot more that goes into evaluating the efficacy of the model (mostly determining \"if our test set a good representation of reality\"), but for our pedagogical purposes this will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3478bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8038201352964585\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.score(test_vectors, y_test['target'])\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d920b",
   "metadata": {},
   "source": [
    "# We get about 80 percent accuracy\n",
    "__If you notice this is better than the tutorial notebook we started from__: https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial . Try and see what I did differently to get a better score than the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39fa4b",
   "metadata": {},
   "source": [
    "## Importance of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad544ed",
   "metadata": {},
   "source": [
    "Notice that because of the numerical embeddings we can take __text__ data and turn it into a mathematical vector to perform statistical analysis via logistic regression. The concept of embeddings plays a crucial role in modern AI. Computers can process only mathemtical expressions, so finding creative ways to embed information that is not numerical can become an art. \n",
    "\n",
    "On top of training decision algorithms on the embeded vectors, there is also interesting transformations that can take place in the embdedded space. For example, if I give a human or an AI algorithm the phrase \"A Red Apple with a Worm inside of it\", both can embed the text data into something abstract, then extract a visual representation of the same information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90f3cf",
   "metadata": {},
   "source": [
    "## Check out other methods at\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/code?competitionId=17777 \n",
    "and bring cool ideas to the next CoP meeting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf605f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
