{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b88fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec6b22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6b398",
   "metadata": {},
   "source": [
    "# Brief look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1194e6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>7934</td>\n",
       "      <td>rainstorm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Calum5SOS you look like you got caught in a r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>2538</td>\n",
       "      <td>collision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my favorite lady came to our volunteer meeting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>2611</td>\n",
       "      <td>crashed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@brianroemmele UX fail of EMV - people want to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6810</th>\n",
       "      <td>9756</td>\n",
       "      <td>tragedy</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Can't find my ariana grande shirt  this is a f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>6254</td>\n",
       "      <td>hijacking</td>\n",
       "      <td>Athens,Greece</td>\n",
       "      <td>The Murderous Story Of AmericaÛªs First Hijac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>2597</td>\n",
       "      <td>crash</td>\n",
       "      <td>In my own world!!!</td>\n",
       "      <td>AKILAH WORLD NEWS Cop pulls man from car to av...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>8711</td>\n",
       "      <td>sinking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We walk the plank of a sinking ship</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5701</th>\n",
       "      <td>8136</td>\n",
       "      <td>rescued</td>\n",
       "      <td>Pennsylvania, USA</td>\n",
       "      <td>@Zak_Bagans pets r like part of the family. I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6029</th>\n",
       "      <td>8618</td>\n",
       "      <td>seismic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ON THE USE OF PERFORATED METAL SHEAR PANEL SFO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>4326</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>CA via Brum</td>\n",
       "      <td>The answer my friend is yelling in the wind-my...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>4456</td>\n",
       "      <td>electrocuted</td>\n",
       "      <td>New York</td>\n",
       "      <td>Woman electrocuted #Red #Redblood #videoclip h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>5710</td>\n",
       "      <td>floods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Who is bringing the tornadoes and floods. Who ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>5218</td>\n",
       "      <td>fatality</td>\n",
       "      <td>NY</td>\n",
       "      <td>So these savages leaked Thomas Brady gangsterm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>3105</td>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Malaysia Airlines Flight 370 that Disappeared ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>7717</td>\n",
       "      <td>panicking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>People are finally panicking about cable TV ht...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword               location  \\\n",
       "2644  3796   destruction                    NaN   \n",
       "2227  3185        deluge                    NaN   \n",
       "5448  7769        police                     UK   \n",
       "132    191    aftershock                    NaN   \n",
       "6845  9810        trauma  Montgomery County, MD   \n",
       "5559  7934     rainstorm                    NaN   \n",
       "1765  2538     collision                    NaN   \n",
       "1817  2611       crashed                    NaN   \n",
       "6810  9756       tragedy        Los Angeles, CA   \n",
       "4398  6254     hijacking          Athens,Greece   \n",
       "1807  2597         crash     In my own world!!!   \n",
       "6101  8711       sinking                    NaN   \n",
       "5701  8136       rescued      Pennsylvania, USA   \n",
       "6029  8618       seismic                    NaN   \n",
       "3012  4326  dust%20storm            CA via Brum   \n",
       "3105  4456  electrocuted               New York   \n",
       "4019  5710        floods                    NaN   \n",
       "3664  5218      fatality                     NY   \n",
       "2164  3105        debris                    NaN   \n",
       "5406  7717     panicking                    NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  \n",
       "5559  @Calum5SOS you look like you got caught in a r...       0  \n",
       "1765  my favorite lady came to our volunteer meeting...       1  \n",
       "1817  @brianroemmele UX fail of EMV - people want to...       1  \n",
       "6810  Can't find my ariana grande shirt  this is a f...       0  \n",
       "4398  The Murderous Story Of AmericaÛªs First Hijac...       1  \n",
       "1807  AKILAH WORLD NEWS Cop pulls man from car to av...       1  \n",
       "6101                We walk the plank of a sinking ship       0  \n",
       "5701  @Zak_Bagans pets r like part of the family. I ...       0  \n",
       "6029  ON THE USE OF PERFORATED METAL SHEAR PANEL SFO...       0  \n",
       "3012  The answer my friend is yelling in the wind-my...       0  \n",
       "3105  Woman electrocuted #Red #Redblood #videoclip h...       0  \n",
       "4019  Who is bringing the tornadoes and floods. Who ...       0  \n",
       "3664  So these savages leaked Thomas Brady gangsterm...       0  \n",
       "2164  Malaysia Airlines Flight 370 that Disappeared ...       1  \n",
       "5406  People are finally panicking about cable TV ht...       0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df2736",
   "metadata": {},
   "source": [
    "# Clean the text up a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38a3db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.lower()) # convert to lowercase\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(\"#\", \"\")) # remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63942f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"http\" not in x])) # remove hypterlinks\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"@\" not in x])) # remove tags\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"a\" != x])) # remove a\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"the\" != x])) # remove the\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([x for x in x.split(\" \") if \"an\" != x])) # remove an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff66ac27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>10224</td>\n",
       "      <td>volcano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hill hill mountain volcano of hell mountain hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>3086</td>\n",
       "      <td>deaths</td>\n",
       "      <td>Blackpool</td>\n",
       "      <td>cancers equate for around 25% of all deaths in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>6247</td>\n",
       "      <td>hijacking</td>\n",
       "      <td>World</td>\n",
       "      <td>murderous story of americaûªs first hijacking...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>3602</td>\n",
       "      <td>desolation</td>\n",
       "      <td>Birmingham, UK</td>\n",
       "      <td>date for release of ep03 desolation is set. st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1987</td>\n",
       "      <td>bush%20fires</td>\n",
       "      <td>London/Bristol/Guildford</td>\n",
       "      <td>on holiday to relax sunbathe and drink ... put...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6825</th>\n",
       "      <td>9775</td>\n",
       "      <td>trapped</td>\n",
       "      <td>????s ?? ????Ìø????Ì¡a</td>\n",
       "      <td>(?eudrylantiqua?) hollywood movie about trappe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>5514</td>\n",
       "      <td>flattened</td>\n",
       "      <td>Some other mansion</td>\n",
       "      <td>flattened all cartoony-like.\\n'whoa there papa!'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>4957</td>\n",
       "      <td>exploded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news science london warship exploded in 1665 b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6245</th>\n",
       "      <td>8921</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>'cooler than freddie jackson sippin' milkshake...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>8083</td>\n",
       "      <td>rescue</td>\n",
       "      <td>Wanderlust</td>\n",
       "      <td>mary coming to troy rescue. ??????</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>5599</td>\n",
       "      <td>flood</td>\n",
       "      <td>New York</td>\n",
       "      <td>spot flood combo 53inch 300w curved cree led w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>4388</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>in the Word of God</td>\n",
       "      <td>thx for your great encouragement and for rt of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>8546</td>\n",
       "      <td>screams</td>\n",
       "      <td>NaN</td>\n",
       "      <td>when you on phone and screams 'jaileens caked ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>10296</td>\n",
       "      <td>weapon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cat of nine irons xii: this nightmarishly brut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>9283</td>\n",
       "      <td>sunk</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>its like i never left. i just sunk to background</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>1185</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>columbus ohio</td>\n",
       "      <td>just order blizzard pay then put your nuts in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>5868</td>\n",
       "      <td>hailstorm</td>\n",
       "      <td>facebook.com/tradcatknights</td>\n",
       "      <td>canada: hailstorm flash flooding slam calgary ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4623</th>\n",
       "      <td>6571</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>incident with injury:i-495  inner loop exit 31...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>1296</td>\n",
       "      <td>bloody</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>bloody insomnia again! grrrr!! insomnia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7450</th>\n",
       "      <td>10662</td>\n",
       "      <td>wounds</td>\n",
       "      <td>The American Wasteland (MV)</td>\n",
       "      <td>-in vault that could take look at those wounds...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>4366</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>a box</td>\n",
       "      <td>'s things she looks in significant other:\\n1. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>612</td>\n",
       "      <td>arsonist</td>\n",
       "      <td>America</td>\n",
       "      <td>if you don't have anything nice to say you can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>273</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>Loveland Colorado</td>\n",
       "      <td>check out what's in my parking lot!! he said t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>788</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Freeport il</td>\n",
       "      <td>possible new jerseys for avalanche next year. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950</th>\n",
       "      <td>9972</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crptotech tsunami and banks.\\n banking tech bi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       keyword                     location  \\\n",
       "7138  10224       volcano                          NaN   \n",
       "2151   3086        deaths                    Blackpool   \n",
       "4395   6247     hijacking                        World   \n",
       "2508   3602    desolation               Birmingham, UK   \n",
       "1378   1987  bush%20fires     London/Bristol/Guildford   \n",
       "6825   9775       trapped       ????s ?? ????Ìø????Ì¡a   \n",
       "3877   5514     flattened           Some other mansion   \n",
       "3465   4957      exploded                          NaN   \n",
       "6245   8921     snowstorm                 Brooklyn, NY   \n",
       "5664   8083        rescue                   Wanderlust   \n",
       "3938   5599         flood                     New York   \n",
       "3059   4388    earthquake           in the Word of God   \n",
       "5985   8546       screams                          NaN   \n",
       "7186  10296        weapon                          NaN   \n",
       "6493   9283          sunk                 San Jose, CA   \n",
       "816    1185      blizzard                columbus ohio   \n",
       "4127   5868     hailstorm  facebook.com/tradcatknights   \n",
       "4623   6571        injury                          NaN   \n",
       "895    1296        bloody                    Leicester   \n",
       "7450  10662        wounds  The American Wasteland (MV)   \n",
       "3043   4366    earthquake                        a box   \n",
       "422     612      arsonist                      America   \n",
       "194     273     ambulance            Loveland Colorado   \n",
       "542     788     avalanche                 Freeport il    \n",
       "6950   9972       tsunami                          NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "7138  hill hill mountain volcano of hell mountain hi...       1  \n",
       "2151  cancers equate for around 25% of all deaths in...       1  \n",
       "4395  murderous story of americaûªs first hijacking...       1  \n",
       "2508  date for release of ep03 desolation is set. st...       1  \n",
       "1378  on holiday to relax sunbathe and drink ... put...       0  \n",
       "6825  (?eudrylantiqua?) hollywood movie about trappe...       1  \n",
       "3877   flattened all cartoony-like.\\n'whoa there papa!'       0  \n",
       "3465  news science london warship exploded in 1665 b...       1  \n",
       "6245  'cooler than freddie jackson sippin' milkshake...       0  \n",
       "5664                 mary coming to troy rescue. ??????       0  \n",
       "3938  spot flood combo 53inch 300w curved cree led w...       0  \n",
       "3059  thx for your great encouragement and for rt of...       1  \n",
       "5985  when you on phone and screams 'jaileens caked ...       0  \n",
       "7186  cat of nine irons xii: this nightmarishly brut...       1  \n",
       "6493   its like i never left. i just sunk to background       0  \n",
       "816   just order blizzard pay then put your nuts in ...       0  \n",
       "4127  canada: hailstorm flash flooding slam calgary ...       1  \n",
       "4623  incident with injury:i-495  inner loop exit 31...       1  \n",
       "895             bloody insomnia again! grrrr!! insomnia       1  \n",
       "7450  -in vault that could take look at those wounds...       0  \n",
       "3043  's things she looks in significant other:\\n1. ...       0  \n",
       "422   if you don't have anything nice to say you can...       0  \n",
       "194   check out what's in my parking lot!! he said t...       0  \n",
       "542   possible new jerseys for avalanche next year. ...       0  \n",
       "6950  crptotech tsunami and banks.\\n banking tech bi...       1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72a5c856",
   "metadata": {},
   "source": [
    "# We must turn text into mathematical representation\n",
    "\n",
    "There are a number of ways to do this, the most simple of which is a count vectorizor, where we simply count the number of times a word shows up in the tweet.\n",
    "\n",
    "\n",
    "For our transformer example, we use a pre-trained tokenizer. This has the vocabulary step already taken care of (the training), and all it has to do is to map our text documents into encoded vectors. We set the max length of any document/tweet to the length of the longest tweet in our training data. We allow truncation of any tweets that are longer than this if one is passed to the model during inference (or training) and we utilize a padding scheme of \"max_length\" to pad shorter tweets until they are the same length of the max_length. This is done because the training step will be performed by matrix algebra, and in order to multiply matrices together they must be a consistent length, e.i. a 3x1 matrix can be multiplied by a 3x3 matrix, but cannot be multiplied by a matrix that has 3 rows of length: 3-2-3, respectively. (that is in fact not a well-defined matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6173f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our deeds are reason of this earthquake may allah forgive us all',\n",
       " 'forest fire near la ronge sask. canada']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[\"text\"][0:2].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c9c3788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/dylan.frizzell/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/dylan.frizzell/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/dylan.frizzell/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/dylan.frizzell/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/dylan.frizzell/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoding = tokenizer(list(df[\"text\"][0:3].values),\n",
    "                     padding=\"max_length\", \n",
    "                     truncation=True,\n",
    "                     max_length=df[\"text\"].apply(len).max())\n",
    "## let's get counts for the first single tweet in the data\n",
    "# example_train_vectors = count_vectorizer.fit_transform(df[\"text\"][0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d55927dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2256, 15616, 2024, 3114, 1997, 2023, 8372, 2089, 16455, 9641, 2149, 2035, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3224, 2543, 2379, 2474, 6902, 3351, 21871, 2243, 1012, 2710, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2035, 3901, 2356, 2000, 1005, 7713, 1999, 2173, 1005, 2024, 2108, 19488, 2011, 3738, 1012, 2053, 2060, 13982, 2030, 7713, 1999, 2173, 4449, 2024, 3517, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show an example encoding. Notice the padding as well as sentence begin and end tokens (101,102).\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58cc8547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] forest fire near la ronge sask. canada [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding['input_ids'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bcddc1",
   "metadata": {},
   "source": [
    "## Notice some things about this new encoding.\n",
    "There is a preceeding 101 and trailing 102 for all rows, these are special start and seperator tokens. There is an additional token type id that is used to signify special sequences (https://huggingface.co/docs/transformers/v4.26.0/en/model_doc/bert#transformers.BertTokenizer). Also there is an attention mask that tells the model which columns to look at. This is because the BERT model requires a square tensor as input, so all input columns must be the same length, and we pad the end with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d3e2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bddafd11",
   "metadata": {},
   "source": [
    "# I am now going to embedded datasets of both train and test separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1040d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_samples = 1000\n",
    "n_test_samples = 50\n",
    "## let's get embeddings for the  data\n",
    "encoded_train_dataset = tokenizer(list(df[\"text\"][:n_training_samples].values),\n",
    "                     padding=\"max_length\",\n",
    "                     truncation=True,\n",
    "                     max_length=df[\"text\"].apply(len).max())\n",
    "\n",
    "encoded_test_dataset = tokenizer(list(df[\"text\"][n_training_samples:n_training_samples+n_test_samples].values),\n",
    "                     padding=\"max_length\",\n",
    "                     truncation=True,\n",
    "                     max_length=df[\"text\"].apply(len).max())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9bff774",
   "metadata": {},
   "source": [
    "## This particular model uses \"labels\" as a keyword for prediction labels.\n",
    "So this means that for our encoded_\\<version\\>_dataset we have to build a \"labels\" column that has the category we are aiming to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bebf464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_dataset['labels'] = df[:n_training_samples]['target'].values.astype(np.int64)\n",
    "encoded_test_dataset['labels'] = df[n_training_samples:n_training_samples+n_test_samples]['target'].values.astype(np.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65695e50",
   "metadata": {},
   "source": [
    "## I use the huggingface \"Dataset\" object to make training a little easier.\n",
    "\n",
    "There is actually quite a bit of difficulty that can be had here to get the data the right \"type\", a Pytorch tensor object of type Float/Long. Using some of this huggingface infrastructure is supposed to help with that... supposed to. Now-a-days I think using the _Dataset_ object from the datasets module is essentially mandatory in order to fine-tune or train models. This creates a \"Dataset\" object that has properties that huggingface knows to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9954649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Value\n",
    "\n",
    "train_dataset = Dataset.from_dict(encoded_train_dataset)\n",
    "test_dataset = Dataset.from_dict(encoded_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3c5e366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc43e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should set the format of our dataset object to pytorch, as this is the underlaying model type of \"Bert\" which we will use\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e47f975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One can see that we should be converting our datatypes into \"torch\" objects.\n",
    "train_dataset['input_ids'].type()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e75e17f",
   "metadata": {},
   "source": [
    "Here is where we actually download and register the ML model of choice. We are using the bert-base model which is a pretrained transformer architecture that is not specialized toward any specific dataset. We use the uncased dataset which refers to the fact that it draws now distinction between upper and lowercase when building tokens. The \"problem_type\" argument tells huggingface what kind of \"head\" to attach to the model. The last few layers of the model can be referred to as the head and give different final responses depending on the desired task. In this case we select \"single_label_classificationn\" which means we are looking at a binary classification problem - a yes or no. This while utilize a final layer that is a single sigmoid function which provides a single number that can be mapped between -1,1 and can be interpretted as a confidence that a document/tweet is a \"yes\" or \"no\", or in our case a tweet about a natural disaster or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4e2d39da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/dylan.frizzell/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/dylan.frizzell/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"single_label_classification\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e16867b",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "I will use some of huggingface's model training infrastructure to generate a training arguments object. AWS Sagemaker has something similar when building training jobs. \n",
    "\n",
    "I should note that I have selected model parameters that are for demonstration purposes only. I am running on a light CPU laptop, so I only perform a single epoch. In a more normal setting, one will want to train for multiple epochs, as well as perform hyperparameter scans on other parameters. This essentially means training multiple models with different arguments and selecting the model that most suitably solves your problem while not displaying any signs of overtraining or undesireable extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac2e315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-binary-class\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "#     metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "000d8828",
   "metadata": {},
   "source": [
    "We can check the training dataset's form to ensure it will be as expected by the traning job. Since we used the tokenizer that was used with the bert-base-uncased it has the embeddings that are expected by the model which is given as input_ids, token_type_ids, and attention_mask. We also have the label column that we created that is unique to our specific task that we are aiming to fine tune on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38e3acf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd55dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the huggingface train object that will run our training jobs.\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    "#     compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be2e61b",
   "metadata": {},
   "source": [
    "# This is a test to see if the model can compute with no errors\n",
    "Before running the training job we need to make sure that our dataset is correctly formatted in the expected format that the model is expecting. This means not only the shape of the training dataframe, but also the datatype, integer/float size, torch/tensorflow tensor type, etc. By running one of our single input_ids through the model and getting a response shows that the datatypes, tensor types, data sizes, etc. are all in alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8d20264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7547, grad_fn=<NllLossBackward0>), logits=tensor([[0.3679, 0.2484]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=train_dataset['input_ids'][0].unsqueeze(0),\n",
    "                labels=train_dataset[0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e661fa12",
   "metadata": {},
   "source": [
    "# We can either train a model or load a pretrained one\n",
    "\n",
    "The first time thorugh this notebook you will want to run the training job. Upon subsequent runs, in order to save time one can upload the model that has already been fine tuned in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fa4ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylan.frizzell/COP/nlp-demo/.venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 300\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 76\n",
      "  Number of trainable parameters = 109483778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d9200e19024a9ca0efc47e75fda7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43e404432284757bc5af25227f82ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-finetuned-binary-class/checkpoint-38\n",
      "Configuration saved in bert-finetuned-binary-class/checkpoint-38/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5303837656974792, 'eval_runtime': 17.4092, 'eval_samples_per_second': 2.872, 'eval_steps_per_second': 0.402, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-finetuned-binary-class/checkpoint-38/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-binary-class/checkpoint-38/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-binary-class/checkpoint-38/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02795a3467c34feabff65addd6439b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-finetuned-binary-class/checkpoint-76\n",
      "Configuration saved in bert-finetuned-binary-class/checkpoint-76/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4777432978153229, 'eval_runtime': 21.3435, 'eval_samples_per_second': 2.343, 'eval_steps_per_second': 0.328, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-finetuned-binary-class/checkpoint-76/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-binary-class/checkpoint-76/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-binary-class/checkpoint-76/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-binary-class/checkpoint-76 (score: 0.4777432978153229).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 832.749, 'train_samples_per_second': 0.721, 'train_steps_per_second': 0.091, 'train_loss': 0.5219881158126028, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=76, training_loss=0.5219881158126028, metrics={'train_runtime': 832.749, 'train_samples_per_second': 0.721, 'train_steps_per_second': 0.091, 'train_loss': 0.5219881158126028, 'epoch': 2.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this line if this is the first time running this notebook or you wish to retune a model.\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1564f7d7",
   "metadata": {},
   "source": [
    "## Model checkpoints are saved\n",
    "As you can see in the above output, the model was trained and model \"checkpoints\" are saved that give the model weights at the end of each epoch. We can then reload the model at that particular moment in time. \n",
    "\n",
    "## Let's do this now to load the model we just trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec235bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-finetuned-binary-class/checkpoint-76/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-finetuned-binary-class/checkpoint-76\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-finetuned-binary-class/checkpoint-76/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-finetuned-binary-class/checkpoint-76.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_trained_model = AutoModelForSequenceClassification.from_pretrained(\"bert-finetuned-binary-class/checkpoint-76\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3378c5",
   "metadata": {},
   "source": [
    "# Finally let's play around with our newly trained model\n",
    "And see how we can tweak words to understand what it has learned. Notice how the order of the words now matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "932a1ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      "tensor([0.2018, 0.1848], grad_fn=<SqueezeBackward0>)\n",
      "------------------------------------\n",
      "Sigmoid: [probability_no, probability_yes]\n",
      "tensor([0.5503, 0.5461], grad_fn=<SigmoidBackward0>)\n",
      "------------------------------------\n",
      "Prediction:\n",
      "tensor(True)\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"There is a very serious emergency and a fire at the lodge.\"\n",
    "# text = \"These chicken wings are fire.\"\n",
    "\n",
    "# text = \"mudslide on my mountain\"\n",
    "# text = \"on my mudslide mountain\"\n",
    "\n",
    "# Take our text snippet and create an embedding in the pytorch(pt) format.\n",
    "# This uses the proper tokenizer from before\n",
    "# We also map this to the proper dataset dictionary format in the second line\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(model.device) for k,v in encoding.items()}\n",
    "\n",
    "# Create the inference\n",
    "outputs = loaded_trained_model(**encoding)\n",
    "\n",
    "# Get the output layers/logits. This is the \"head\" of the model's raw output.\n",
    "# There are two logits, one for \"no\" and one for \"yes\" in regards to \"is this a tweet about a natural disaster?\"\n",
    "logits = outputs.logits\n",
    "logits.shape\n",
    "\n",
    "# We look at the output of the logits, and apply a squeeze normalization. \n",
    "# Again, this is essentially the raw output of the model's response of [\"no\",\"yes\"]\n",
    "print(\"Logits:\")\n",
    "print(logits.squeeze().cpu())\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# In order to attach a probabilistic interpretation to this, we run the logits through the sigmoid function to map \n",
    "# The logits' values to probabilities. We are left with a tensor that is interpretted as [percent belief no, percent belief yes].\n",
    "# The order of no,yes is set by the order of our 'labels' column in the train/test datasets (we labeled 'no' tweet as label 0, 'yes' as 1).\n",
    "# Depending on the training method and dataset, these do not always sum to 1. For this we must calibrate the model.\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "print(\"Sigmoid: [probability_no, probability_yes]\")\n",
    "print(probs)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Lastly if we must make a decision we will say our prediction is tied to whether or not the \"yes\" column\n",
    "# has a greater than 50% confidence.\n",
    "print(\"Prediction:\")\n",
    "print(probs[1]>0.5)\n",
    "print(\"------------------------------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a194b20",
   "metadata": {},
   "source": [
    "##  Am aside on logits and sigmoid functions\n",
    "BERT uses the BCEwithLogits loss function and the output of the model is actual not a prediction like standard cross-entropy loss functions. In order to acheive the probabilistic interpretation we use the sigmoid function.\n",
    "\n",
    "Sigmoid:     $\\sigma = \\frac{1}{1-e^{\\beta x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b4fcd",
   "metadata": {},
   "source": [
    "![](sigmoid.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a40ac838",
   "metadata": {},
   "source": [
    "# We can calculate predictions on the full test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fd9e258e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i, tweet in enumerate(df[\"text\"][n_training_samples:n_training_samples+n_test_samples].values):\n",
    "    # print(\"Calculating\")\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\")\n",
    "    outputs = loaded_trained_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions.append(probs)\n",
    "    # print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9cfac75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.3415, 0.7190], grad_fn=<SigmoidBackward0>),\n",
       " tensor([0.5845, 0.3669], grad_fn=<SigmoidBackward0>),\n",
       " tensor([0.6335, 0.2735], grad_fn=<SigmoidBackward0>)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0f45041",
   "metadata": {},
   "source": [
    "# Check the accuracy\n",
    "Now that we have a series of predictions on the test set we can use this to estimate the accurcy of our model.\n",
    "This is a little more difficult with the difference in output of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "19512256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "is_dangerous = [1 if x < 0.5 else 0 for x in [x[0] for x in predictions] ]\n",
    "test_labels = df[n_training_samples:n_training_samples+n_test_samples]['target'].values.astype(np.int64)\n",
    "\n",
    "accuracy = accuracy_score(is_dangerous, test_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
